{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- 感知机是根据输入实例的特征向量$\\rm x$ 对其进行二分类的线性分类模型\n",
    "\n",
    "- 感知机模型：\n",
    "$$ f(x) = sign(w\\cdot x + b) $$\n",
    "感知机模型对应于输入空间（特征空间）中的分离超平面$w\\cdot x + b = 0 $\n",
    "\n",
    "- 感知机学习的策略是极小化损失函数：\n",
    "$$ \\underset{w,b}min \\ L(w,b) = - \\sum_{x_i \\in M}{y_i(w x_i + b)}$$\n",
    "损失函数对英语误分类点到分离超平面的总距离。\n",
    "\n",
    "- 感知机的学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式。\n",
    "\n",
    "- 当训练数据集线性可分时，感知机学习算法是收敛的。感知机算法在训练数据集上的误分类次数k满足不等式：\n",
    "\n",
    "$$ k <\\left(\\cfrac{R}{\\gamma}\\right)^2$$\n",
    "\n",
    "- 当训练数据集线性可分时，感知机学习算法存在无穷多解，其解由于不同的初值或不同的迭代顺序而可能有所不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Definition\n",
    "\n",
    "假设输入空间（特征空间）是 $ \\chi \\in R^n $, 输出空间是Y = { +1, -1 }, 输入 $ x \\in \\chi $ 表示实例的特征向量，对应于输入空间（特征空间）的点；\n",
    "输出 $ y \\in Y $表示实例的类别。由输入空间到输出空间的如下函数\n",
    "$$ f(x) = sign(\\omega \\cdot x + b)         \\; (2.1)$$\n",
    "\n",
    "称为**感知机**。 其中$ \\omega 和 b $称为**感知机模型参数**， $ \\omega \\in R^n $ 叫做 **权值(weight) 或权值向量(weight vector)**, $ b \\in R^n $叫做 **偏置(bias)**， $ \\omega \\cdot x 表示 \\omega 和 x的内积$。 sign是符号函数，即：\n",
    "\n",
    "$$\n",
    "sign(x) = \n",
    "\\begin{cases}\n",
    "+1,&x \\geq 0 \\\\\n",
    "-1,&x<0 \n",
    "\\end{cases}\n",
    " \\; (2.2)\n",
    "$$\n",
    "\n",
    "感知机是一种**线性分类模型，属于判别模型**。 感知机模型的假设空间是定义在**特征空间中的所有线性分类模型(linear classification model) 或线性分类器(linear classifier)**, 即函数集合$ \\lbrace f \\mid f(x) = \\omega \\cdot x + b \\rbrace $\n",
    "\n",
    "#### 线性方程的解释\n",
    "线性方程\n",
    "$$ \\omega \\cdot x + b =0 $$\n",
    "对应于特征空间$ R^n $ 中的一个超平面S， 其中$ \\omega $是超平面的**法向量**, b 是超平面的**截距**。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。因此，**超平面S** 称为**分离超平面(separating hyperplane)**。\n",
    "\n",
    "**特征空间也就是整个n维空间，样本的每个属性都叫一个特征**，特征空间的意思是在这个空间中可以找到样本所有的属性组合。\n",
    "\n",
    "**感知机学习**，由训练数据集（实例的特征向量及类别）\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots , (x_N, y_N)\\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ 求得感知机模型（2.1），即求得模型参数$\\omega, b$。\n",
    "\n",
    "**感知机预测**，通过学习得到得感知机模型，对于新得输入实例给出其对应得输出类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集的线性可分性\n",
    "\n",
    "定义2.2 给定一个数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ ,如果存在某个超平面S\n",
    "\n",
    "$$ \\omega \\cdot x + b = 0 $$\n",
    "\n",
    "能够将数据集的正实例点和负实例点正确地划分到超平面的两侧，即对所有$y_i=+1$的实例$ \\mathit i$， 有$ \\omega \\cdot x + b > 0 $, 所有$y_i=-1$的实例i， 有$ \\omega \\cdot x + b < 0 $, 称数据集T为**线性可分数据集(linear separable data set) **, 否则，称为线性不可分。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的学习策略\n",
    "\n",
    "假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。\n",
    "\n",
    "为了找出这样的超平面，即**确定感知机模型参数 $ \\omega , b $**, 需要确定一个**学习策略，即定义（经验）损失函数，并将损失函数极小化**。\n",
    "\n",
    "- 损失函数的**一个自然选择是误分类点的总数**。但是，这样的损失函数不是参数$ \\omega, b$的连续可导函数，不易优化。\n",
    "- 另一个选择是**误分类点到超平面S的函数间距，即：**，\n",
    "$$ \\sum_{i=0}^n \\mid \\omega \\cdot x_i + b \\mid $$\n",
    "**函数间距的缺陷**在于等比例放大缩小$ \\omega, b $后，**超平面并没有改变，但是函数间距等比例放大或缩小了**。\n",
    "\n",
    "- 所以，常用的是**几何间距，即：**\n",
    "$$ \\cfrac {1}{\\parallel \\omega \\parallel _{2}} \\sum_{i=0}^n \\mid \\omega \\cdot x_i + b \\mid $$\n",
    "其中，\n",
    "$$ \\parallel \\omega \\parallel _2 = \\sqrt {\\sum_{i=1}^N w_i^2 } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机学习算法 之 原始形式\n",
    "\n",
    "对于误分类数据而言：$ -y_i (\\omega \\cdot x + b) > 0 $\n",
    "\n",
    "**误分类点$x_i$到超平面S的距离为**\n",
    "$$ -\\cfrac {1}{\\parallel \\omega \\parallel} y_i (\\omega \\cdot x + b) $$\n",
    "\n",
    "因此，所有误分类点到超平面S的总距离为：\n",
    "$$ - \\cfrac {1}{\\parallel \\omega \\parallel} \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b)$$\n",
    "\n",
    "其中，假设超平面S的误分类点的集合为M。\n",
    "#因为$ \\cfrac {1}{\\parallel \\omega \\parallel} $为固定值，可以不考虑，就得到感知机学习的损失函数。\n",
    "因为感知机学习目的是**没有一个误分类点**， 因此，不存在等比例缩小距离的问题，这里可以用函数间隔而不是集合间隔。\n",
    "\n",
    "给定训练数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ 。\n",
    "感知机$ sign(\\omega \\cdot x + b) $ 歇息的损失函数定义为：\n",
    "$$ L(\\omega，b) = - \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) \\;\\;\\;\\;\\;\\;\\;\\; (2.4)$$\n",
    "\n",
    "其中M为误分类点的集合。**这个损失函数就是感知机学习的经验风险函数**。\n",
    "\n",
    "一个特定的样本点的损失函数：**在误分类时是参数$ \\omega, b$的线性函数，在正确分类时是0**。\n",
    "因此，**给定训练数据集T，损失函数$ L(\\omega, b)$ 是 $ \\omega, b $的连续可导函数**。\n",
    "\n",
    "**感知机学习的策略是在假设空间中选取使损失函数式（2.4）最小的模型参数$ \\omega, b$, 即感知机模型**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机学习算法 之 原始形式\n",
    "感知机学习算法使对以下最优化问题的算法。\n",
    "\n",
    "给定训练数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ 。\n",
    "求参数$\\omega, b $, 使其为以下损失函数极小化问题的解\n",
    "$$ min_{\\omega, b} L(\\omega, b) = -\\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) $$\n",
    "\n",
    "其中 M为误分类点的集合。\n",
    "\n",
    "\n",
    "感知机学习算法是**误分类驱动的**，不存在等比例缩小参数使得距离缩小的问题，因此，可以用函数间隔而不是几何间隔。\n",
    "当然用几何间隔也可以，但是多了一步计算量。\n",
    "\n",
    "具体采用**随机梯度下降法(stochastic gradient descent, SGD)**\n",
    "\n",
    "- 1. 任选取超平面$\\omega_0, b_0 $\n",
    "- 2. 采用梯度下降法极小化目标函数\n",
    "$$ L(\\omega, b) = -\\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) $$\n",
    "$$ \\nabla_{\\omega} L(\\omega, b) = - \\sum_{x_i \\in M} y_i x_i   \\;\\;\\;针对\\omega 求偏导$$\n",
    "$$ \\nabla_{b} L(\\omega, b) = - \\sum_{x_i \\in M} y_i \\;\\;\\;针对b求偏导$$\n",
    "\n",
    "- 3. 更新$\\omega, b$\n",
    "$$ \\omega \\leftarrow \\omega + \\eta y_i x_i $$\n",
    "$$ b \\leftarrow b+ \\eta y_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机算法的收敛性 Perceptron-Algorithm Convergence\n",
    "\n",
    "需要证明，感知机的原始算法在**线性可分数据集上**收敛。\n",
    "\n",
    "便于推导，将偏置b并入$\\omega $,记作 $\\hat \\omega = (\\omega^T, b^T) $,同样也将输入向量加以扩充，加入常数1， 记作 $ \\hat x = (x^T, 1) $, 这样处理后，有$ \\hat \\omega \\cdot \\rm{x} = \\omega \\cdot \\rm{x} + b $\n",
    "\n",
    "**定理（2.1）（Novikoff）**\n",
    "\n",
    "设训练数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$ 是**线性可分的**\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ ， 则\n",
    "\n",
    "（1） 存在满足条件$ \\parallel \\hat\\omega_\\mathit{opt} \\parallel = 1$ 的超平面 $ \\hat\\omega_\\mathit{opt} \\cdot \\hat{x} = \\omega_\\mathit{opt} \\cdot + b = 0 $将训练集完全真确分开；且存在$ \\gamma > 0 $ ,对所有$\\mathit{i} = 1,2,\\cdots, N $\n",
    "$$ y_i(\\hat\\omega_\\mathit{opt} \\cdot \\hat x_i) = y_i(\\omega_\\mathit{opt} \\cdot x_i + b_\\mathit{opt}) \\geq \\gamma   \\;\\;\\;(2.8) $$\n",
    "\n",
    "(2) 令 $ R = \\underset{1\\leq\\mathit{i}\\leq{N}}{max} \\parallel \\hat{x_i} \\parallel $, 则感知机算法2.1在训练集上的误分类次数$ \\mathit{k} $ 满足不等式\n",
    "$$ \\mathit{k} \\leq \\left( \\cfrac{R}{\\gamma} \\right)^2    \\;\\;\\; (2.9)$$\n",
    "\n",
    "证明见书P42-43\n",
    "\n",
    "定理表明，**误分类的次数k是有上界的，经过有限次搜索可以找到将训练数据集完全正确分开的分离超平面**。\n",
    "\n",
    "但是例子2.1说明，感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。\n",
    "\n",
    "**为了得到唯一的超平面，需要对分离超平面增加约束条件，就是*线性支持向量机*的想法**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的对偶形式 Perceptron Dual\n",
    "\n",
    "对感知机算法原始形式的思考：\n",
    "\n",
    "- 每次参数的更新公式为：\n",
    "$$ \\omega \\leftarrow \\omega + \\eta{y_i x_i} $$\n",
    "$$ b \\leftarrow b + \\eta{y_i} $$\n",
    "\n",
    "- 每次按照上式更新，假设修改$ \\rm{n} $次，那么对于样本点$ (x_i, y_i) $而言，$ \\omega, b $的增量为$ \\alpha_i{y_i x_i} 和 \\alpha_i{y_i} $,其中， $ \\alpha_i =n_i \\eta $\n",
    "$$ \\omega = \\sum_{i=1}^N \\alpha_i{y_i x_i }    \\;\\;\\; (2.14) $$\n",
    "$$ b = \\sum_{i=1}^N \\alpha_i{y_i}    \\;\\;\\; (2.15)$$\n",
    "\n",
    "- 感知机的原始形式为:\n",
    "$$ \\mathit{f(x) = sign(\\omega \\cdot x + b) } $$\n",
    "\n",
    "- 将(2.14), (2.15)带入感知机原始形式中：\n",
    "$$ \\mathit{ f(x) = sign(\\sum_{i=1}^N{\\alpha_i y_i x_i \\cdot x + b) } } $$\n",
    "\n",
    "就引入了感知机算法的对偶形式\n",
    "\n",
    "**算法2.2 （感知机学习算法的对偶形式)**\n",
    "\n",
    "输入： **线性可分数据集**\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N ，学习率 \\eta, (0\\leq\\eta\\leq 1) $ ;\n",
    "输出： \\$ \\omega, b $; 感知机模型 $ \\mathit{ f(x) = sign(\\sum_{i=1}^N{\\alpha_i y_i x_i \\cdot x + b) } } $，其中，\n",
    "$ \\alpha = (\\alpha_1, \\alpha_2, \\cdots, \\alpha_N)^T $\n",
    "\n",
    "(1) $ \\alpha \\leftarrow 0, b \\leftarrow 0 $  **对偶形式中，$\\alpha, b$初始值必须为0， 这点和原始形式不一样**。因为在最初始时，没有误分类点 $ \\mathit{n_i} = 0 $;\n",
    "\n",
    "(2) 在训练集中选取数据$(x_i, y_i)$;\n",
    "\n",
    "(3) 如果$ y_i \\left(\\sum_{j=1}^N {\\alpha_j y_i x_j \\cdot x_i + b }\\right) \\leq 0, $\n",
    "\n",
    "$$ \\alpha_i \\leftarrow \\alpha_i + \\eta $$\n",
    "$$ b \\leftarrow b + \\eta y_i $$\n",
    "\n",
    "(4) 转至（2）知道没有误分类点。\n",
    "\n",
    "**对偶形式中训练实例仅以内积的形式出现 ( 既，$ x_j \\cdot x_i $， 这是一个固定值)**。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个形式就是**Gram 矩阵（Gram Matrix）**。\n",
    "$$ \\boldsymbol{G} = \\left [ x_i \\cdot x_j \\right]_{\\mathbf N \\times \\mathbf N} $$\n",
    "\n",
    "这样就会使计算过程简化到O(1)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ - \\cfrac {1}{\\parallel \\omega \\parallel} \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b)$$\n",
    "$$ \\sum_{x_i \\in M} $$\n",
    "$$ \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
