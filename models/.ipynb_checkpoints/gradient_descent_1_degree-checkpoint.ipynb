{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descend (1) - 1-degree polynomial \n",
    "\n",
    "https://www.youtube.com/watch?v=vsWrXfO3wWw\n",
    "\n",
    "$ y_{i\\_predicted} = \\omega * x_i + b $\n",
    "\n",
    "Cost Function(MSE) $ C = \\cfrac{1}{n}\\sum_{i=1}^n{(y_i - y_{i\\_predicted})} $\n",
    "\n",
    "$ \\cfrac{\\partial C}{\\partial \\omega} = \\cfrac{2}{n}\\sum_{i=1}^n{-x_i(y_i - y_{i\\_predicted})} $\n",
    "\n",
    "$ \\cfrac{\\partial C}{\\partial b} = \\cfrac{2}{n}\\sum_{i=1}^n{-(y_i - y_{i\\_predicted})} $\n",
    "\n",
    "Update $\\omega$ and b\n",
    "\n",
    "$ \\omega_{new} = \\omega - \\eta \\cdot \\cfrac{\\partial C}{\\partial \\omega} $\n",
    "\n",
    "$ b_{new} = b - \\eta \\cdot \\cfrac{\\partial C}{\\partial b} $\n",
    "\n",
    "in which, $ \\eta$ = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m [4.96], b [1.44], cost [89.] iteration 0\n",
      "m [0.4992], b [0.2688], cost [71.1056] iteration 1\n",
      "m [4.451584], b [1.426176], cost [56.82977024] iteration 2\n",
      "m [0.89223168], b [0.50122752], cost [45.43965676] iteration 3\n",
      "m [4.04131471], b [1.43275991], cost [36.35088702] iteration 4\n",
      "m [1.20087606], b [0.70368726], cost [29.09748333] iteration 5\n",
      "m [3.70956431], b [1.45467679], cost [23.30787285] iteration 6\n",
      "m [1.44248627], b [0.88133764], cost [18.68575876] iteration 7\n",
      "m [3.44066837], b [1.48793021], cost [14.9948676] iteration 8\n",
      "m [1.63088554], b [1.03834056], cost [12.04678724] iteration 9\n",
      "m [3.22212352], b [1.52938101], cost [9.69126935] iteration 10\n",
      "m [1.77708324], b [1.17806076], cost [7.80849683] iteration 11\n",
      "m [3.04394758], b [1.57657108], cost [6.30291812] iteration 12\n",
      "m [1.88984572], b [1.30322487], cost [5.09833084] iteration 13\n",
      "m [2.89816931], b [1.62758294], cost [4.13396168] iteration 14\n",
      "m [1.97615151], b [1.4160484], cost [3.36134053] iteration 15\n",
      "m [2.77842162], b [1.68092793], cost [2.74180805] iteration 16\n",
      "m [2.04155416], b [1.51833709], cost [2.24452823] iteration 17\n",
      "m [2.67961704], b [1.73545716], cost [1.84490367] iteration 18\n",
      "m [2.09047162], b [1.61156783], cost [1.52331192] iteration 19\n",
      "m [2.59768901], b [1.7902906], cost [1.26409791] iteration 20\n",
      "m [2.12641686], b [1.69695338], cost [1.05477044] iteration 21\n",
      "m [2.52938556], b [1.84476075], cost [0.88536155] iteration 22\n",
      "m [2.15218181], b [1.77549396], cost [0.74791565] iteration 23\n",
      "m [2.47210472], b [1.89836765], cost [0.63608209] iteration 24\n",
      "m [2.16998394], b [1.84801856], cost [0.54479038] iteration 25\n",
      "m [2.4237633], b [1.9507433], cost [0.46999111] iteration 26\n",
      "m [2.18158311], b [1.91521799], cost [0.4084494] iteration 27\n",
      "m [2.3826922], b [2.00162322], cost [0.35758015] iteration 28\n",
      "m [2.18837478], b [1.97767125], cost [0.31531668] iteration 29\n",
      "m [2.34755297], b [2.05082395], cost [0.28000599] iteration 30\n",
      "m [2.19146425], b [2.0358667], cost [0.25032517] iteration 31\n",
      "m [2.31727116], b [2.09822519], cost [0.22521482] iteration 32\n",
      "m [2.19172583], b [2.090219], cost [0.20382584] iteration 33\n",
      "m [2.29098325], b [2.14375556], cost [0.18547709] iteration 34\n",
      "m [2.18985006], b [2.14108271], cost [0.16962157] iteration 35\n",
      "m [2.26799425], b [2.18738145], cost [0.15581941] iteration 36\n",
      "m [2.18638127], b [2.18876318], cost [0.14371641] iteration 37\n",
      "m [2.24774391], b [2.22909806], cost [0.13302684] iteration 38\n",
      "m [2.18174756], b [2.23352529], cost [0.12351973] iteration 39\n",
      "m [2.22977971], b [2.26892242], cost [0.11500796] iteration 40\n",
      "m [2.17628466], b [2.27560057], cost [0.10733953] iteration 41\n",
      "m [2.21373539], b [2.30688784], cost [0.10039059] iteration 42\n",
      "m [2.17025494], b [2.3151928], cost [0.09405986] iteration 43\n",
      "m [2.1993137], b [2.34303958], cost [0.08826424] iteration 44\n",
      "m [2.16386259], b [2.35248267], cost [0.08293518] iteration 45\n",
      "m [2.18627275], b [2.3774314], cost [0.07801592] iteration 46\n",
      "m [2.15726564], b [2.38763146], cost [0.07345918] iteration 47\n",
      "m [2.17441502], b [2.41012292], cost [0.06922534] iteration 48\n",
      "m [2.15058559], b [2.42078404], cost [0.06528102] iteration 49\n",
      "m [2.16357861], b [2.44117751], cost [0.06159788] iteration 50\n",
      "m [2.14391505], b [2.45207138], cost [0.05815172] iteration 51\n",
      "m [2.1536303], b [2.47066073], cost [0.05492168] iteration 52\n",
      "m [2.13732382], b [2.48161247], cost [0.05188972] iteration 53\n",
      "m [2.14445991], b [2.49863904], cost [0.04904004] iteration 54\n",
      "m [2.13086373], b [2.50951604], cost [0.04635878] iteration 55\n",
      "m [2.13597587], b [2.52517888], cost [0.04383363] iteration 56\n",
      "m [2.12457247], b [2.53588185], cost [0.04145362] iteration 57\n",
      "m [2.12810163], b [2.55034596], cost [0.03920889] iteration 58\n",
      "m [2.1184767], b [2.56080182], cost [0.03709057] iteration 59\n",
      "m [2.12077283], b [2.57420472], cost [0.03509055] iteration 60\n",
      "m [2.11259438], b [2.584361], cost [0.03320146] iteration 61\n",
      "m [2.11393499], b [2.59681794], cost [0.03141652] iteration 62\n",
      "m [2.1069368], b [2.60663827], cost [0.02972951] iteration 63\n",
      "m [2.10754166], b [2.61824649], cost [0.02813465] iteration 64\n",
      "m [2.10151002], b [2.62770705], cost [0.0266266] iteration 65\n",
      "m [2.101553], b [2.63854911], cost [0.02520038] iteration 66\n",
      "m [2.09631615], b [2.64763582], cost [0.02385134] iteration 67\n",
      "m [2.09593454], b [2.65778233], cost [0.02257516] iteration 68\n",
      "m [2.09135423], b [2.66648858], cost [0.02136776] iteration 69\n",
      "m [2.09065626], b [2.67600038], cost [0.02022534] iteration 70\n",
      "m [2.08662106], b [2.68432531], cost [0.01914432] iteration 71\n",
      "m [2.08569185], b [2.69325515], cost [0.01812134] iteration 72\n",
      "m [2.08211172], b [2.70120224], cost [0.01715323] iteration 73\n",
      "m [2.08101801], b [2.70959626], cost [0.01623699] iteration 74\n",
      "m [2.07782011], b [2.71717221], cost [0.01536983] iteration 75\n",
      "m [2.07661406], b [2.72507101], cost [0.01454909] iteration 76\n",
      "m [2.07373923], b [2.7322849], cost [0.01377225] iteration 77\n",
      "m [2.07246143], b [2.73972448], cost [0.01303696] iteration 78\n",
      "m [2.06986156], b [2.74658708], cost [0.01234098] iteration 79\n",
      "m [2.06854342], b [2.7535996], cost [0.01168219] iteration 80\n",
      "m [2.0661792], b [2.76012282], cost [0.01105861] iteration 81\n",
      "m [2.06484486], b [2.76673715], cost [0.01046833] iteration 82\n",
      "m [2.06268407], b [2.77293368], cost [0.00990959] iteration 83\n",
      "m [2.06135194], b [2.77917593], cost [0.00938069] iteration 84\n",
      "m [2.05936808], b [2.78505885], cost [0.00888002] iteration 85\n",
      "m [2.05805201], b [2.79095276], cost [0.0084061] iteration 86\n",
      "m [2.05622315], b [2.79653535], cost [0.00795747] iteration 87\n",
      "m [2.05493344], b [2.80210259], cost [0.00753279] iteration 88\n",
      "m [2.05324135], b [2.80739812], cost [0.00713078] iteration 89\n",
      "m [2.05198548], b [2.81265858], cost [0.00675023] iteration 90\n",
      "m [2.05041492], b [2.81768017], cost [0.00639] iteration 91\n",
      "m [2.04919818], b [2.82265218], cost [0.00604899] iteration 92\n",
      "m [2.04773634], b [2.82741271], cost [0.00572618] iteration 93\n",
      "m [2.04656228], b [2.83211323], cost [0.0054206] iteration 94\n",
      "m [2.04519831], b [2.83662522], cost [0.00513133] iteration 95\n",
      "m [2.04406918], b [2.84107], cost [0.00485749] iteration 96\n",
      "m [2.04279383], b [2.84534559], cost [0.00459828] iteration 97\n",
      "m [2.04171081], b [2.84954926], cost [0.00435289] iteration 98\n",
      "m [2.04051614], b [2.85360019], cost [0.0041206] iteration 99\n",
      "m [2.03947964], b [2.85757641], cost [0.00390071] iteration 100\n",
      "m [2.0383588], b [2.86141396], cost [0.00369255] iteration 101\n",
      "m [2.03736862], b [2.8651755], cost [0.0034955] iteration 102\n",
      "m [2.03631561], b [2.86881049], cost [0.00330896] iteration 103\n",
      "m [2.0353711], b [2.87236932], cost [0.00313238] iteration 104\n",
      "m [2.03438069], b [2.8758121], cost [0.00296523] iteration 105\n",
      "m [2.03348087], b [2.87917943], cost [0.00280699] iteration 106\n",
      "m [2.03254841], b [2.8824399], cost [0.0026572] iteration 107\n",
      "m [2.03169205], b [2.88562628], cost [0.0025154] iteration 108\n",
      "m [2.03081343], b [2.88871389], cost [0.00238117] iteration 109\n",
      "m [2.02999913], b [2.89172922], cost [0.0022541] iteration 110\n",
      "m [2.02917063], b [2.89465297], cost [0.00213381] iteration 111\n",
      "m [2.02839689], b [2.89750659], cost [0.00201994] iteration 112\n",
      "m [2.0276152], b [2.90027502], cost [0.00191215] iteration 113\n",
      "m [2.02688044], b [2.90297573], cost [0.00181011] iteration 114\n",
      "m [2.02614252], b [2.905597], cost [0.00171352] iteration 115\n",
      "m [2.02544513], b [2.90815307], cost [0.00162208] iteration 116\n",
      "m [2.02474823], b [2.91063492], cost [0.00153552] iteration 117\n",
      "m [2.02408658], b [2.91305418], cost [0.00145358] iteration 118\n",
      "m [2.02342819], b [2.91540395], cost [0.00137601] iteration 119\n",
      "m [2.02280068], b [2.91769379], cost [0.00130258] iteration 120\n",
      "m [2.02217847], b [2.91991846], cost [0.00123307] iteration 121\n",
      "m [2.02158351], b [2.92208584], cost [0.00116727] iteration 122\n",
      "m [2.02099533], b [2.92419202], cost [0.00110498] iteration 123\n",
      "m [2.02043138], b [2.92624354], cost [0.00104601] iteration 124\n",
      "m [2.01987525], b [2.92823751], cost [0.00099019] iteration 125\n",
      "m [2.0193408], b [2.93017939], cost [0.00093735] iteration 126\n",
      "m [2.01881488], b [2.9320671], cost [0.00088733] iteration 127\n",
      "m [2.01830848], b [2.93390522], cost [0.00083998] iteration 128\n",
      "m [2.01781105], b [2.93569232], cost [0.00079516] iteration 129\n",
      "m [2.01733129], b [2.93743224], cost [0.00075272] iteration 130\n",
      "m [2.01686074], b [2.93912406], cost [0.00071255] iteration 131\n",
      "m [2.01640628], b [2.94077106], cost [0.00067453] iteration 132\n",
      "m [2.01596112], b [2.94237267], cost [0.00063853] iteration 133\n",
      "m [2.01553067], b [2.94393171], cost [0.00060446] iteration 134\n",
      "m [2.01510947], b [2.94544791], cost [0.0005722] iteration 135\n",
      "m [2.0147018], b [2.9469237], cost [0.00054167] iteration 136\n",
      "m [2.01430325], b [2.94835904], cost [0.00051276] iteration 137\n",
      "m [2.01391719], b [2.94975603], cost [0.0004854] iteration 138\n",
      "m [2.01354004], b [2.95111482], cost [0.0004595] iteration 139\n",
      "m [2.01317446], b [2.95243723], cost [0.00043498] iteration 140\n",
      "m [2.01281754], b [2.95372353], cost [0.00041176] iteration 141\n",
      "m [2.01247137], b [2.95497535], cost [0.00038979] iteration 142\n",
      "m [2.01213359], b [2.95619303], cost [0.00036899] iteration 143\n",
      "m [2.01180581], b [2.95737802], cost [0.0003493] iteration 144\n",
      "m [2.01148613], b [2.95853075], cost [0.00033066] iteration 145\n",
      "m [2.01117578], b [2.95965249], cost [0.00031301] iteration 146\n",
      "m [2.01087321], b [2.96074371], cost [0.00029631] iteration 147\n",
      "m [2.01057938], b [2.96180558], cost [0.0002805] iteration 148\n",
      "m [2.010293], b [2.96283858], cost [0.00026553] iteration 149\n",
      "m [2.0100148], b [2.96384377], cost [0.00025136] iteration 150\n",
      "m [2.00974374], b [2.96482166], cost [0.00023795] iteration 151\n",
      "m [2.00948036], b [2.9657732], cost [0.00022525] iteration 152\n",
      "m [2.00922379], b [2.96669892], cost [0.00021323] iteration 153\n",
      "m [2.00897444], b [2.96759967], cost [0.00020185] iteration 154\n",
      "m [2.00873158], b [2.96847599], cost [0.00019108] iteration 155\n",
      "m [2.00849552], b [2.96932867], cost [0.00018088] iteration 156\n",
      "m [2.00826564], b [2.97015824], cost [0.00017123] iteration 157\n",
      "m [2.00804216], b [2.97096541], cost [0.00016209] iteration 158\n",
      "m [2.00782456], b [2.97175071], cost [0.00015344] iteration 159\n",
      "m [2.00761299], b [2.97251481], cost [0.00014525] iteration 160\n",
      "m [2.00740702], b [2.9732582], cost [0.0001375] iteration 161\n",
      "m [2.00720673], b [2.97398152], cost [0.00013016] iteration 162\n",
      "m [2.00701176], b [2.97468525], cost [0.00012322] iteration 163\n",
      "m [2.00682215], b [2.97536996], cost [0.00011664] iteration 164\n",
      "m [2.00663759], b [2.97603614], cost [0.00011042] iteration 165\n",
      "m [2.00645809], b [2.97668432], cost [0.00010453] iteration 166\n",
      "m [2.00628338], b [2.97731494], cost [9.89485945e-05] iteration 167\n",
      "m [2.00611346], b [2.97792853], cost [9.366832e-05] iteration 168\n",
      "m [2.00594808], b [2.9785255], cost [8.8669821e-05] iteration 169\n",
      "m [2.00578722], b [2.97910635], cost [8.3938061e-05] iteration 170\n",
      "m [2.00563067], b [2.97967147], cost [7.94588058e-05] iteration 171\n",
      "m [2.00547839], b [2.98022131], cost [7.52185807e-05] iteration 172\n",
      "m [2.0053302], b [2.98075627], cost [7.12046302e-05] iteration 173\n",
      "m [2.00518604], b [2.98127678], cost [6.74048795e-05] iteration 174\n",
      "m [2.00504576], b [2.98178319], cost [6.38078979e-05] iteration 175\n",
      "m [2.00490929], b [2.98227592], cost [6.0402865e-05] iteration 176\n",
      "m [2.0047765], b [2.98275531], cost [5.71795376e-05] iteration 177\n",
      "m [2.00464731], b [2.98322174], cost [5.41282193e-05] iteration 178\n",
      "m [2.00452161], b [2.98367555], cost [5.1239731e-05] iteration 179\n",
      "m [2.00439931], b [2.9841171], cost [4.85053834e-05] iteration 180\n",
      "m [2.00428032], b [2.98454669], cost [4.59169511e-05] iteration 181\n",
      "m [2.00416455], b [2.98496467], cost [4.34666474e-05] iteration 182\n",
      "m [2.0040519], b [2.98537134], cost [4.11471012e-05] iteration 183\n",
      "m [2.00394231], b [2.98576701], cost [3.89513349e-05] iteration 184\n",
      "m [2.00383568], b [2.98615198], cost [3.6872743e-05] iteration 185\n",
      "m [2.00373194], b [2.98652654], cost [3.49050728e-05] iteration 186\n",
      "m [2.00363099], b [2.98689096], cost [3.30424049e-05] iteration 187\n",
      "m [2.00353278], b [2.98724553], cost [3.12791361e-05] iteration 188\n",
      "m [2.00343723], b [2.98759051], cost [2.9609962e-05] iteration 189\n",
      "m [2.00334426], b [2.98792616], cost [2.80298615e-05] iteration 190\n",
      "m [2.0032538], b [2.98825273], cost [2.65340811e-05] iteration 191\n",
      "m [2.0031658], b [2.98857047], cost [2.51181213e-05] iteration 192\n",
      "m [2.00308017], b [2.98887961], cost [2.37777225e-05] iteration 193\n",
      "m [2.00299686], b [2.98918039], cost [2.25088525e-05] iteration 194\n",
      "m [2.0029158], b [2.98947303], cost [2.13076942e-05] iteration 195\n",
      "m [2.00283694], b [2.98975776], cost [2.01706344e-05] iteration 196\n",
      "m [2.0027602], b [2.99003479], cost [1.90942524e-05] iteration 197\n",
      "m [2.00268555], b [2.99030433], cost [1.80753102e-05] iteration 198\n",
      "m [2.00261291], b [2.99056657], cost [1.71107427e-05] iteration 199\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.__version__\n",
    "\n",
    "def gradient_descent(x,y):\n",
    "    m_curr = b_curr = 0\n",
    "    iterations = 200\n",
    "    n = len(x)\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n",
    "        md = -(2/n)*sum(x*(y-y_predicted))\n",
    "        bd = -(2/n)*sum(y-y_predicted)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        print (\"m {}, b {}, cost {} iteration {}\".format(m_curr,b_curr,cost, i))\n",
    "\n",
    "x = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "y = np.array([5,7,9,11,13]).reshape(5,1)\n",
    "\n",
    "gradient_descent(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent  - 1-degree polynomial (2)\n",
    "\n",
    "https://www.youtube.com/watch?v=sDv4f4s2SB8\n",
    "\n",
    "**Step 1** Take the derivative of the ***Loss Function*** for each parameter in it.\n",
    "In fancy Machine Learnin Lingo, take the **Gradient** of Loss Function.\n",
    "\n",
    "**step 2** Pick random values for the parameters\n",
    "\n",
    "**step 3** Plug the parameter values into the derivatives.\n",
    "\n",
    "**step 4** Calculate the step sizes: step_size = slope * learning_rate\n",
    "\n",
    "**step 5** Calculate the new parameters:\n",
    "New Parameter = Old Parameter - step_size\n",
    "\n",
    "**go back to step 3**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
