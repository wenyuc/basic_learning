{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Definition\n",
    "\n",
    "假设输入空间（特征空间）是 $ \\chi \\in R^n $, 输出空间是Y = { +1, -1 }, 输入 $ x \\in \\chi $ 表示实例的特征向量，对应于输入空间（特征空间）的点；\n",
    "输出 $ y \\in Y $表示实例的类别。由输入空间到输出空间的如下函数\n",
    "$$ f(x) = sign(\\omega \\cdot x + b)         \\; (2.1)$$\n",
    "\n",
    "称为**感知机**。 其中$ \\omega 和 b $称为**感知机模型参数**， $ \\omega \\in R^n $ 叫做 **权值(weight) 或权值向量(weight vector)**, $ b \\in R^n $叫做 **偏置(bias)**， $ \\omega \\cdot x 表示 \\omega 和 x的内积$。 sign是符号函数，即：\n",
    "\n",
    "$$\n",
    "sign(x) = \n",
    "\\begin{cases}\n",
    "+1,&x \\geq 0 \\\\\n",
    "-1,&x<0 \n",
    "\\end{cases}\n",
    " \\; (2.2)\n",
    "$$\n",
    "\n",
    "感知机是一种**线性分类模型，属于判别模型**。 感知机模型的假设空间是定义在**特征空间中的所有线性分类模型(linear classification model) 或线性分类器(linear classifier)**, 即函数集合$ \\lbrace f \\mid f(x) = \\omega \\cdot x + b \\rbrace $\n",
    "\n",
    "#### 线性方程的解释\n",
    "线性方程\n",
    "$$ \\omega \\cdot x + b =0 $$\n",
    "对应于特征空间$ R^n $ 中的一个超平面S， 其中$ \\omega $是超平面的**法向量**, b 是超平面的**截距**。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。因此，**超平面S** 称为**分离超平面(separating hyperplane)**。\n",
    "\n",
    "**特征空间也就是整个n维空间，样本的每个属性都叫一个特征**，特征空间的意思是在这个空间中可以找到样本所有的属性组合。\n",
    "\n",
    "**感知机学习**，由训练数据集（实例的特征向量及类别）\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots , (x_N, y_N)\\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ 求得感知机模型（2.1），即求得模型参数$\\omega, b$。\n",
    "\n",
    "**感知机预测**，通过学习得到得感知机模型，对于新得输入实例给出其对应得输出类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集的线性可分性\n",
    "\n",
    "定义2.2 给定一个数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ ,如果存在某个超平面S\n",
    "\n",
    "$$ \\omega \\cdot x + b = 0 $$\n",
    "\n",
    "能够将数据集的正实例点和负实例点正确地划分到超平面的两侧，即对所有$y_i=+1$的实例i， 有$ \\omega \\cdot x + b > 0 $, 所有$y_i=-1$的实例i， 有$ \\omega \\cdot x + b < 0 $, 称数据集T为**线性可分数据集(linear separable data set) **, 否则，称为线性不可分。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的学习策略\n",
    "\n",
    "假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。\n",
    "\n",
    "为了找出这样的超平面，即**确定感知机模型参数 $ \\omega , b $**, 需要确定一个**学习策略，即定义（经验）损失函数，并将损失函数极小化**。\n",
    "\n",
    "- 损失函数的**一个自然选择是误分类点的总数**。但是，这样的损失函数不是参数$ \\omega, b$的连续可到函数，不易优化。\n",
    "- 另一个选择是**误分类点到超平面S的函数间距，即：**，\n",
    "$$ \\sum_{i=0}^n \\mid \\omega \\cdot x_i + b \\mid $$\n",
    "**函数间距的缺陷**在于等比例放大缩小$ \\omega, b $后，**超平面并没有改变，但是函数间距等比例放大或缩小了**。\n",
    "\n",
    "- 所以，常用的是**几何间距，即：**\n",
    "$$ \\cfrac {1}{\\parallel \\omega \\parallel _{2}} \\sum_{i=0}^n \\mid \\omega \\cdot x_i + b \\mid $$\n",
    "其中，\n",
    "$$ \\parallel \\omega \\parallel _2 = \\sqrt {\\sum_{i=1}^N w_i^2 } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
