{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing AdaBoosting in Python\n",
    "\n",
    "https://www.youtube.com/watch?v=7xHM93WXOu8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 李航 “统计学习方法” 第八章 提升方法\n",
    "\n",
    "#### 提升方法的基本思路\n",
    "\n",
    "- 将弱可学习算法提升为强可学习算法；\n",
    "- 其中提升方法是集成学习的一种；\n",
    "- 集成学习的两个主要类别：\n",
    "    - 序列方法\n",
    "    - 并行方法\n",
    "\n",
    "- XGBoost : 逐步减小偏差，串行计算，逐步减小偏差\n",
    "- Bagging : 逐步减小方差，并行计算，最后投票\n",
    "\n",
    "$$ \\sigma^2 = \\rho\\sigma^2 + (1-\\rho)\\cfrac{\\sigma^2}{n} $$\n",
    "\n",
    "其中，$\\rho$为相关系数，因为每次为随机抽取的数据，$\\rho\\rightarrow0$，所以方差为原来$\\cfrac{1}{n}$\n",
    "\n",
    "而对于串行算法，$\\rho\\rightarrow1$， 因此，对方差而言，没有显著的提升。\n",
    "\n",
    "#### Adaboost算法\n",
    "输入： 训练数据集$ T = \\{(x_1,y_1),(x_2,y_2),\\cdots, (x_N,y_N)\\} $, 其中，$ x_i \\in X\\subseteq R^n, y_i \\in Y = \\{-1, 1\\} $;弱学习算法。\n",
    "\n",
    "输出：最终分类器$ G_\\left(x\\right) $\n",
    "\n",
    "(1) 初始化训练数据的权值分布\n",
    "\n",
    "$$ D_1 = (\\omega_{11},\\cdots, \\omega_{1i}, \\cdots, \\omega_{1N}), \\omega_{1i}=\\cfrac{1}{N}, i=1,2,\\cdots,N $$\n",
    "\n",
    "(2) 对 $m=1,2,\\cdots,M$\n",
    "\n",
    " (a) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器：\n",
    "\n",
    "$$ G_m(x): X \\rightarrow \\{-1,1\\} $$\n",
    "\n",
    " (b) 计算$G_m(x)$在训练数据集上的分类误差率：\n",
    " \n",
    "$$ e_m = \\sum_{i=1}^N{P(G_m(x_i)\\neq y_i)} = \\sum_{i=1}^N{\\omega_i I(G_m(x_i)\\neq y_i)} $$\n",
    "\n",
    " (c) 计算$G_m(x)$在训练数据集上的分类误差：\n",
    "\n",
    "$$ \\alpha_m = \\cfrac{1}{2}log\\cfrac{1-e_m}{e_m} $$\n",
    "\n",
    " (d) 更新训练数据集的权值分布：\n",
    " \n",
    "$$ D_{m+1} = (\\omega_{m+1,1}, \\omega_{m+1,2},\\cdots, \\omega_{m+1,i},\\cdots, \\omega_{m+1,N}) $$\n",
    "\n",
    "$$ \\omega_{m+1,i} = \\cfrac{\\omega_{mi}}{Z_m} exp(-\\alpha_m y_i G_m(x_i)) $$\n",
    "\n",
    "可以写为\n",
    "$$\n",
    "\\omega_{m+1,i} = \n",
    "\\begin{cases}\n",
    "\\cfrac{\\omega_{mi}}{Z_m}e^{-\\alpha_m},&G_m(x_i)=y_i \\\\\n",
    "\\cfrac{\\omega_{mi}}{Z_m}e^{\\alpha_m},&G_m(x_i)\\neq y_i \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "这里，$Z_m$是规范化因子，它使$D_{m+1}$成为一个概率分布：\n",
    "\n",
    "$$ Z_m = \\sum_{i=1}^N{\\omega_{mi} exp(-\\alpha_m y_i G_m(x_i))} $$\n",
    "\n",
    "(3) 构建基本分类器的先行组合\n",
    "\n",
    "$$ f(x) = \\sum_{m=1}^M{\\alpha_m G_m(x)} $$\n",
    "\n",
    "得到最终分类器\n",
    "\n",
    "$$ G(x) = sign(f(x)) \n",
    "        = sign(\\sum_{m=1}^M{\\alpha_m G_m(x)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提升树（Boosting Tree）\n",
    "\n",
    "基本分类器: 分类树或回归树\n",
    "\n",
    "提升树模型：\n",
    "\n",
    "$$ f_M(x) = \\sum_{m=1}^M{T(x;\\theta_m)} $$\n",
    "\n",
    "前向分步算法：\n",
    "\n",
    "$$ f_m(x) = f_{m-1}(x)+T(x;\\theta_m) $$\n",
    "\n",
    "$$ \\hat\\theta_m = argmin \\sum_{i=1}^N{L(y_i,f_{m-1}(x_i)+T(x;\\theta_m))} $$\n",
    "\n",
    "\n",
    "$$ f_0(x) = 0 $$\n",
    "\n",
    "$$ f_1(x) = f_0(x)+T(x;\\theta_1) $$\n",
    "\n",
    "$$ \\cfrac{1}{N}\\sum_{i=1}{N}{L(y_i,f_1(x_i))} $$\n",
    "\n",
    "对于回归问题，一般为\n",
    "\n",
    "$$ L(y_i,f_1(x_i)) = \\cfrac{1}{2}(y-f(x))^2 $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归问题（平方误差损失）：\n",
    "算法8.3 (回归问题的提升树方法）\n",
    "\n",
    "输入： 训练数据集$ T = \\{(x_1,y_1),(x_2,y_2),\\cdots, (x_N,y_N)\\} $, 其中，$ x_i \\in X\\subseteq R^n, y_i \\in Y \\subseteq R_i;$\n",
    "\n",
    "输出： 提升树$f_M(x)$\n",
    "\n",
    "(1) 初始化$f_0(x)=0$\n",
    "\n",
    "(2)对 $ m=1,2,\\cdots, M$\n",
    "\n",
    "(a)计算残差 \n",
    "\n",
    "$$ \\gamma_{mi} = y_i - f_{m-1}(x_i), i=1,2,\\cdots,N $$\n",
    "\n",
    "(b)拟合残差$\\gamma_{mi}$ 学习一个回归树$ T(x;\\theta_m) $\n",
    "\n",
    "(c) 更新$f_m(x) = f_{m-1}(x)+T(x;\\theta_m) $\n",
    "\n",
    "(3)得到回归问题提升树：\n",
    "\n",
    "$$ f_M(x) = \\sum_{m=1}^M{T(x;\\theta_m)} $$\n",
    "\n",
    "拟合残差的原因：\n",
    "\n",
    "对于任意的样本点y和拟合值f(x)的损失：\n",
    "\n",
    "$$ L[y,f(x)] = [y-f(x)]^2 $$\n",
    "\n",
    "在前项分布算法中：\n",
    "\n",
    "$$ f_m(x) = [y-f_{m-1}(x) - T(x;\\theta_m)]^2 \\\\\n",
    "          = [\\gamma_{m-1} - T(x;\\theta_m)]^2 \\\\\n",
    "          = L(\\gamma_{m-1},T(x;\\theta_m)) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**回归问题梯度提升：**\n",
    "\n",
    "算法8.4：（梯度提升算法）\n",
    "\n",
    "输入：训练数据集$ T = \\{(x_1,y_1),(x_2,y_2),\\cdots, (x_N,y_N)\\} $, 其中，$ x_i \\in X\\subseteq R^n, y_i \\in Y \\subseteq R; 损失函数L(y,f(x))$\n",
    "\n",
    "输出：回归树$\\hat f(x) $\n",
    "\n",
    "(1)初始化\n",
    "\n",
    "$$ f_0(x) = arg\\;\\underset{c}min \\sum_{i=1}^N{L(x_i,c)} $$\n",
    "\n",
    "(2) 对于$m=1,2,\\cdots, M $\n",
    "\n",
    "    (a)对$i=1,2,\\cdots,N,$计算：\n",
    "\n",
    "$$ \\gamma_{mi} = -\\left[\\cfrac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}\\right]_{f(x)=f_{m-1}(x)} $$\n",
    "\n",
    "对于平方损失函数而言，导数就是残差。对一般损失函数而言，是残差的近似值。\n",
    "\n",
    "(b) 对$\\gamma_{mi}$拟合一个回归树，得到第m棵树的叶节点区域$R_{mj},j=1,2,\\cdots, J $\n",
    "    \n",
    "(c) 对$ j=1,2,\\cdots, J$,计算：\n",
    "    \n",
    "$$ c_{mj} = \\mathit arg\\; \\rm\\underset{c} min \\sum_{x_i \\in R_{mj}}{L(y_i,f_{m-1}(x_i) +c)} $$\n",
    "\n",
    "(d)更新$ f_m(x) = f_{m-1}(x) + \\sum_{j=1}^J{c_{mj}I(x\\in R_{mj})} $\n",
    "\n",
    "（3）得到回归树\n",
    "\n",
    "$$ \\hat f(x) = f_M(x) = \\sum_{m=1}^M\\sum_{j=1}^J{c_{mj}I(x\\in R_{mj})}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "1. 提升算法采用了多个弱模型结合达到一个强模型的效果（减少偏差）。\n",
    "2. AdaBoost每次训练的分类器将重点关注于之前仍然被错分的样本。\n",
    "3. 以决策树为基函数的提升方法被称为提升树。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
