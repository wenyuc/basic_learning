{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Definition\n",
    "\n",
    "假设输入空间（特征空间）是 $ \\chi \\in R^n $, 输出空间是Y = { +1, -1 }, 输入 $ x \\in \\chi $ 表示实例的特征向量，对应于输入空间（特征空间）的点；\n",
    "输出 $ y \\in Y $表示实例的类别。由输入空间到输出空间的如下函数\n",
    "$$ f(x) = sign(\\omega \\cdot x + b)         \\; (2.1)$$\n",
    "\n",
    "称为**感知机**。 其中$ \\omega 和 b $称为**感知机模型参数**， $ \\omega \\in R^n $ 叫做 **权值(weight) 或权值向量(weight vector)**, $ b \\in R^n $叫做 **偏置(bias)**， $ \\omega \\cdot x 表示 \\omega 和 x的内积$。 sign是符号函数，即：\n",
    "\n",
    "$$\n",
    "sign(x) = \n",
    "\\begin{cases}\n",
    "+1,&x \\geq 0 \\\\\n",
    "-1,&x<0 \n",
    "\\end{cases}\n",
    " \\; (2.2)\n",
    "$$\n",
    "\n",
    "感知机是一种**线性分类模型，属于判别模型**。 感知机模型的假设空间是定义在**特征空间中的所有线性分类模型(linear classification model) 或线性分类器(linear classifier)**, 即函数集合$ \\lbrace f \\mid f(x) = \\omega \\cdot x + b \\rbrace $\n",
    "\n",
    "#### 线性方程的解释\n",
    "线性方程\n",
    "$$ \\omega \\cdot x + b =0 $$\n",
    "对应于特征空间$ R^n $ 中的一个超平面S， 其中$ \\omega $是超平面的**法向量**, b 是超平面的**截距**。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。因此，**超平面S** 称为**分离超平面(separating hyperplane)**。\n",
    "\n",
    "**特征空间也就是整个n维空间，样本的每个属性都叫一个特征**，特征空间的意思是在这个空间中可以找到样本所有的属性组合。\n",
    "\n",
    "**感知机学习**，由训练数据集（实例的特征向量及类别）\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots , (x_N, y_N)\\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ 求得感知机模型（2.1），即求得模型参数$\\omega, b$。\n",
    "\n",
    "**感知机预测**，通过学习得到得感知机模型，对于新得输入实例给出其对应得输出类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集的线性可分性\n",
    "\n",
    "定义2.2 给定一个数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ ,如果存在某个超平面S\n",
    "\n",
    "$$ \\omega \\cdot x + b = 0 $$\n",
    "\n",
    "能够将数据集的正实例点和负实例点正确地划分到超平面的两侧，即对所有$y_i=+1$的实例$ \\mathit i$， 有$ \\omega \\cdot x + b > 0 $, 所有$y_i=-1$的实例i， 有$ \\omega \\cdot x + b < 0 $, 称数据集T为**线性可分数据集(linear separable data set) **, 否则，称为线性不可分。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的学习策略\n",
    "\n",
    "假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。\n",
    "\n",
    "为了找出这样的超平面，即**确定感知机模型参数 $ \\omega , b $**, 需要确定一个**学习策略，即定义（经验）损失函数，并将损失函数极小化**。\n",
    "\n",
    "- 损失函数的**一个自然选择是误分类点的总数**。但是，这样的损失函数不是参数$ \\omega, b$的连续可到函数，不易优化。\n",
    "- 另一个选择是**误分类点到超平面S的函数间距，即：**，\n",
    "$$ \\sum_{i=0}^n \\mid \\omega \\cdot x_i + b \\mid $$\n",
    "**函数间距的缺陷**在于等比例放大缩小$ \\omega, b $后，**超平面并没有改变，但是函数间距等比例放大或缩小了**。\n",
    "\n",
    "- 所以，常用的是**几何间距，即：**\n",
    "$$ \\cfrac {1}{\\parallel \\omega \\parallel _{2}} \\sum_{i=0}^n \\mid \\omega \\cdot x_i + b \\mid $$\n",
    "其中，\n",
    "$$ \\parallel \\omega \\parallel _2 = \\sqrt {\\sum_{i=1}^N w_i^2 } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机学习算法 之 原始形式\n",
    "\n",
    "对于误分类数据而言：$ -y_i (\\omega \\cdot x + b) > 0 $\n",
    "\n",
    "**误分类点$x_i$到超平面S的距离为**\n",
    "$$ -\\cfrac {1}{\\parallel \\omega \\parallel} y_i (\\omega \\cdot x + b) $$\n",
    "\n",
    "因此，所有误分类点到超平面S的总距离为：\n",
    "$$ - \\cfrac {1}{\\parallel \\omega \\parallel} \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b)$$\n",
    "\n",
    "其中，假设超平面S的误分类点的集合为M。\n",
    "#因为$ \\cfrac {1}{\\parallel \\omega \\parallel} $为固定值，可以不考虑，就得到感知机学习的损失函数。\n",
    "因为感知机学习目的是**没有一个误分类点**， 因此，不存在等比例缩小距离的问题，这里可以用函数间隔而不是集合间隔。\n",
    "\n",
    "给定训练数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ 。\n",
    "感知机$ sign(\\omega \\cdot x + b) $ 歇息的损失函数定义为：\n",
    "$$ L(\\omega，b) = - \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) \\;\\;\\;\\;\\;\\;\\;\\; (2.4)$$\n",
    "\n",
    "其中M为误分类点的集合。**这个损失函数就是感知机学习的经验风险函数**。\n",
    "\n",
    "一个特定的样本点的损失函数：**在误分类时是参数$ \\omega, b$的线性函数，在正确分类时是0**。\n",
    "因此，**给定训练数据集T，损失函数$ L(\\omega, b)$ 是 $ \\omega, b $的连续可导函数**。\n",
    "\n",
    "**感知机学习的策略是在假设空间中选取使损失函数式（2.4）最小的模型参数$ \\omega, b$, 即感知机模型**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机学习算法 之 原始形式\n",
    "感知机学习算法使对以下最优化问题的算法。\n",
    "\n",
    "给定训练数据集\n",
    "$$ T = \\lbrace (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N) \\rbrace $$\n",
    "\n",
    "其中，$ x_i \\in \\chi = R^n, y_i \\in Y=\\lbrace-1, +1\\rbrace, i = 1,2, \\cdots, N $ 。\n",
    "求参数$\\omega, b $, 使其为以下损失函数极小化问题的解\n",
    "$$ min_{\\omega, b} L(\\omega, b) = -\\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) $$\n",
    "\n",
    "其中 M为误分类点的集合。\n",
    "\n",
    "\n",
    "感知机学习算法是**误分类驱动的**，不存在等比例缩小参数使得距离缩小的问题，因此，可以用函数间隔而不是几何间隔。\n",
    "当然用几何间隔也可以，但是多了一步计算量。\n",
    "\n",
    "具体采用**随机梯度下降法(stochastic gradient descent, SGD)**\n",
    "\n",
    "- 1. 任选取超平面$\\omega_0, b_0 $\n",
    "- 2. 采用梯度下降法极小化目标函数\n",
    "$$ L(\\omega, b) = -\\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) $$\n",
    "$$ \\nabla_{\\omega} L(\\omega, b) = - \\sum_{x_i \\in M} y_i x_i   \\;\\;\\;针对\\omega 求偏导$$\n",
    "$$ \\nabla_{b} L(\\omega, b) = - \\sum_{x_i \\in M} y_i \\;\\;\\;针对b求偏导$$\n",
    "\n",
    "- 3. 更新$\\omega, b$\n",
    "$$ \\omega \\leftarrow \\omega + \\eta y_i x_i $$\n",
    "$$ b \\leftarrow b+ \\eta y_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ - \\cfrac {1}{\\parallel \\omega \\parallel} \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b)$$\n",
    "$$ \\sum_{x_i \\in M} $$\n",
    "$$ \\sum_{x_i \\in M} y_i(\\omega \\cdot x_i + b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
