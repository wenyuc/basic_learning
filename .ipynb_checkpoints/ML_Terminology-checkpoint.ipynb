{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Vector Machine (SVM)\n",
    "\n",
    "https://www.youtube.com/watch?v=efR1C6CvhmE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "https://www.youtube.com/watch?v=EuBBz3bI-aA\n",
    "\n",
    "- The inability for a machine learning method (like linear regression) to\n",
    "capture the true relationship is called **bias**. Because the **Straight Line** can't be curved like the \"true\" relationship, it has a relatively large amount of **bias**.\n",
    "\n",
    "- Another machine learning method might fit a **Squiggly Line** to the training set... The **Squiggly Line** is super flexible and hugs the training set along the arc of the true relationship. The **Squiggly Line** has very little **bias**.\n",
    "\n",
    "- In Machine Learning lingo, the difference in fits between training sets and test sets is called **variance**.\n",
    "\n",
    "- The **Squiggly Line** has **low bias**, since ist is flexible and can adapt to the curve in the relationship between weight and hight. In contrast, the **Straight Line** has relatively high bias, since it cannot capture the curve in the relationship between the weight and height...but the **Straight Line** has relatively **low variance**, because the Sums of Squares are very similar for different datasets. \n",
    "\n",
    "- In other words, the **Straight Line** might only give good predictions, and not great predictions. But they will be **consistently** good predictions.\n",
    "\n",
    "- Three commonly used methods for finding the sweet spot between simple and complicated models are: **regularization, boosting, and bagging**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "https://www.youtube.com/watch?v=fSytzGwwBVw\n",
    "\n",
    "- **Cross Validation** allows us to compare different machine learning methods and get a sense of how well they will work in practice.\n",
    "\n",
    "- Using machine learning lingo, we need data to:\n",
    "    - **Train** the machine learning methods.\n",
    "    - **Test** the machine learning methods.\n",
    "\n",
    "- Rather than worry too much about which set of data would be best for testing, **cross validation** uses them all, one at a time, and summarizes the results at the end. \n",
    "\n",
    "- In this case, since the **support vector machine** did the best job classifying the test datasets, we'll use it!.\n",
    "    - Four-Fold Cross Validation, Ten-Fold Cross Validation\n",
    "    - We could use 10-fold cross validation to help find the best value for that tuning parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and AUC\n",
    "\n",
    "https://www.youtube.com/watch?v=4jRBRDbJemM\n",
    "\n",
    "- When we're doing **Logistic Regression**, the y-axis is converted to the probability that a mouse is obese. So this **Logistic Regression** tells us the ***probability*** that a mouse is obese based on its weight.\n",
    "\n",
    "    - However, if we want to *classify* the mice as obese or not obese, then we need a way to turn probabilities into classifications, such as setting up a threshold at 0.5.\n",
    "\n",
    "- Now we create a **confusion matrix** to summarize the classifications.\n",
    "![jupyter](./figs/confusion_matrix.png)\n",
    "\n",
    "    - Once the **Confusion Matrix** is filled in, we can calculate **Sensitivity and Specificity** to evaluate this **Logistic Regression** when 0.5 is the threshold for obesity.\n",
    "    \n",
    "    - For example, if it was super important to correctly classify every **obese** sample, we could set the threshold to 0.1 ...\n",
    "        - The lower threshold would increase the number of **False-Positive (I area)**.\n",
    "        - The lower threshold would also reduce the number of **False-Negatives(III area)**, because all of the **obese** mice were correctly classified.\n",
    "        - and it would reduce the number of **True Negative(IV are)**, because two of the mice that were not obese were incorrectly classified as obese.\n",
    "        \n",
    "    - **In some cases, it's absolutely essential to correctly classify *every* sample infected with DISEASE in order to minimize the risk of an outbreak**.   ** and that means lowering the threshold, even if that results in more *False Positives* **.\n",
    "    \n",
    "    - With **higher threshold**, this data does a better job classifying samples as obese and not-obese.\n",
    "\n",
    "**How to decide which threshold is the best**\n",
    "\n",
    "- Instead of being overwhelmed with confusion matrices, **Receiver Operator Characteristics(ROC)** graphs provide a simple way to summarize all of the information.\n",
    "\n",
    "> The Y-axis shows the **True Positive Rate**\n",
    "> True Positive Rate = Sensitivity = True Positive / (True Positives + False Negatives)\n",
    "\n",
    "![jupyter](./figs/confusion_matrix2.png)\n",
    "\n",
    "> The **True Positive Rate** tells you what proportion of obese samples were **correctly** classified.\n",
    "\n",
    "\n",
    "> The X-axis shows the **False Positive Rate**, which is the same thing as (1- **Specificity**)\n",
    "\n",
    "> False Positive Rate = (1- Specificity) = False Positives / (False Positives + True Negatives)\n",
    "\n",
    "![jupyter](./figs/confusion_matrix3.png)\n",
    "\n",
    "> The **False Positive Rate** tells you the proportion of not obese samples that were **incorrectly** classified and are False Positives.\n",
    "\n",
    "\n",
    "- The **ROC graph** summarizes all of the confusion matrics that each threshold produced\n",
    "\n",
    "![jupyter](./figs/confusion_matrix4.png)\n",
    "\n",
    "- The **AUC (Area Undr the Curve)** makes it easy to compare one **ROC** curve to another. \n",
    "\n",
    "- Although **ROC** graphs are drawn using **True Positive Rates** and **False Positive Rates** to summarize confusion matrices, there are other metrics that attempt to do the same thing.\n",
    "    - People often replace the *False Positive Rate** with ** Precision**.\n",
    "    > Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "![jupyter](./figs/confusion_matrix5.png)\n",
    "    > Precision is the proportion of positive results that were correctly classified.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
