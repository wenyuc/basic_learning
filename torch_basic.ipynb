{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的创建\n",
    "\n",
    "#### 直接创建\n",
    "torch.tensor()\n",
    "功能：从data创建tensor\n",
    "参数：\n",
    "data: 数据，可以是list, numpy\n",
    "\n",
    "dtype: 数据类型，默认与data一致\n",
    "\n",
    "device: 所在设备, cuda/cpu\n",
    "\n",
    "requires_grad: 是否需要梯度\n",
    "\n",
    "pin_memory: 是否存于锁页内存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "arr = np.ones((3,4))\n",
    "print(arr.dtype)\n",
    "t = torch.tensor(arr)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从numpy创建tensor\n",
    "torch.from_numpy(ndarray)\n",
    "\n",
    "功能：从numpy创建tensor\n",
    "\n",
    "从torch.from_numpy()创建的tensor与原ndarray**共享内存**，当修改其中的一个数据，另一个也修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2,3], [4,5,6],[7,8,9]])\n",
    "t = torch.from_numpy(arr)\n",
    "arr\n",
    "type(arr)\n",
    "t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100,   2,   3],\n",
       "       [  4,   5,   6],\n",
       "       [  7,   8,   9]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[100,   2,   3],\n",
       "        [  4,   5,   6],\n",
       "        [  7,   8,   9]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0,0]=100\n",
    "arr\n",
    "t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100,   2,   3],\n",
       "        [  4,   5,   6],\n",
       "        [  7,   8, 200]], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[100,   2,   3],\n",
       "       [  4,   5,   6],\n",
       "       [  7,   8, 200]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[-1,-1] = 200\n",
    "t\n",
    "arr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 依据数值创建\n",
    "torch.zeros()\n",
    "\n",
    "功能：依size创建全0张量\n",
    "\n",
    "参数：\n",
    "\n",
    "size: 张量的形状，如(3,3), (3, 224, 224)\n",
    "\n",
    "out: 输出的张量\n",
    "\n",
    "dtype: 数据类型\n",
    "\n",
    "layout: 内存中的布局形式，有strided, sparse_coo等\n",
    "\n",
    "device: cpu/cuda\n",
    "\n",
    "requires_grad 是否需要梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698378581312 2698378581312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_t = torch.tensor([1])\n",
    "# out_t = out_t * 2\n",
    "t = torch.zeros((3,3), out=out_t)\n",
    "t \n",
    "print(id(t), id(out_t))\n",
    "id(t) == id(out_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.zeros_like()**\n",
    "\n",
    "功能：依input形状创建全0张量\n",
    "\n",
    "input：创建与input同形状的全0张量\n",
    "\n",
    "dtype: 数据类型\n",
    "\n",
    "layout: 内存中布局形式\n",
    "\n",
    "device:\n",
    "\n",
    "requires_grad:\n",
    "\n",
    "**torch.ones()**\n",
    "\n",
    "**torch.ones_like()**\n",
    "\n",
    "**torch.full()**\n",
    "\n",
    "**torch.full_like()**\n",
    "\n",
    "参数：\n",
    "\n",
    "size: 张量的形状，如(3,3)\n",
    "\n",
    "fill_value: 张量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[55, 55, 55, 55, 55],\n",
       "        [55, 55, 55, 55, 55],\n",
       "        [55, 55, 55, 55, 55],\n",
       "        [55, 55, 55, 55, 55],\n",
       "        [55, 55, 55, 55, 55]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.full((5,5), 55)\n",
    "t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.arange()**\n",
    "\n",
    "功能： 创建等差的**1维张量**\n",
    "\n",
    "数值区间为\\[start, end\\), step 默认为1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(2,20,2)\n",
    "t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.linspace()\n",
    "\n",
    "功能：创建均分数列\n",
    "\n",
    "数值区间为\\[start,end\\], steps为数列长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0000,  3.6364,  5.2727,  6.9091,  8.5455, 10.1818, 11.8182, 13.4545,\n",
       "        15.0909, 16.7273, 18.3636, 20.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.linspace(2,20,12)\n",
    "t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.logspace()**\n",
    "\n",
    "功能：创建对数均分的**1维张量**\n",
    "\n",
    "数值区间\\[start,end\\], 数列长度为steps,底为base，默认为10\n",
    "\n",
    "**torch.eye()**\n",
    "\n",
    "功能：创建单位对角矩阵**(2维方阵)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 依概率分布创建张量\n",
    "\n",
    "torch.normal()\n",
    "\n",
    "生成正态分布\n",
    "\n",
    "mean: 均值\n",
    "\n",
    "std: 标准差\n",
    "\n",
    "四种模式：\n",
    "mean 标量 std 标量\n",
    "\n",
    "mean 标量 std 张量\n",
    "\n",
    "mean 张量 std 标量\n",
    "\n",
    "mean 张量 std 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:tensor([1., 2., 3., 4.])\t std:tensor([1., 2., 3., 4.])\n",
      "tensor([1.8900, 6.5149, 3.4097, 2.7031])\n"
     ]
    }
   ],
   "source": [
    "# mean: 张量 std: 张量\n",
    "mean = torch.arange(1,5, dtype = torch.float)\n",
    "std = torch.arange(1,5, dtype = torch.float)\n",
    "# out_t = torch.tensor([0.,1.,2.,3.])\n",
    "t_normal = torch.normal(mean, std) \n",
    "print(\"mean:{}\\t std:{}\".format(mean, std))\n",
    "print(t_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5645,  0.1384, -0.0393,  0.4202])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean 标量 std 标量, 此时要加一个参数size\n",
    "\n",
    "t_normal = torch.normal(0., 1., size=(4,))\n",
    "t_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0873,  3.5278,  3.7007,  3.7487])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean 张量， std 标量\n",
    "\n",
    "mean = torch.arange(1,5, dtype = torch.float)\n",
    "std = 1\n",
    "t_normal = torch.normal(mean, std)\n",
    "t_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.randn()**\n",
    "\n",
    "**torch.randn_like()**\n",
    "\n",
    "功能: 生成标准正态分布\n",
    "\n",
    "参数： \n",
    "size: 张量的形状\n",
    "\n",
    "\n",
    "**torch.rand()**\n",
    "\n",
    "**torch.rand_like()**\n",
    "\n",
    "功能：在区间\\[0,1\\)上，生成**均匀分布**\n",
    "\n",
    "**torch.randint()**\n",
    "\n",
    "**torch.randint_like()**\n",
    "\n",
    "功能：在区间\\[low,high\\)上，生成**整数均匀分布**\n",
    "\n",
    "size： 张量的形状\n",
    "\n",
    "**torch.randperm(n)**\n",
    "\n",
    "功能：生成从0到n-1的随机数列\n",
    "\n",
    "n: 张量的长度\n",
    "\n",
    "**torch.bernoulli()**\n",
    "\n",
    "功能：以input维概率，生成伯努利分布（0-1分布，两点分布）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的操作\n",
    "\n",
    "#### 拼接与切分\n",
    "\n",
    "**torch.cat()**\n",
    "\n",
    "将张量按维度dim进行拼接\n",
    "\n",
    "tensors: 张量序列\n",
    "\n",
    "dim: 要拼接的维度\n",
    "\n",
    "**torch.stack()**\n",
    "\n",
    "在**在新创建的维度dim**上进行拼接\n",
    "\n",
    "tensors: 张量序列\n",
    "\n",
    "dim: 要拼接的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = torch.zeros((2,3))\n",
    "t1 = torch.ones((2,3))\n",
    "\n",
    "t_0 = torch.cat([t0,t1, t0], dim=0)\n",
    "t_1 = torch.cat([t0,t1, t0], dim=1)\n",
    "t_0\n",
    "t_0.shape\n",
    "t_1\n",
    "t_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros((2,3))\n",
    "\n",
    "t_stack = torch.stack([t,t], dim=0)\n",
    "t_stack\n",
    "t_stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.chunk()**\n",
    "\n",
    "将张量按维度dim进行平均切分\n",
    "\n",
    "返回张量列表\n",
    "\n",
    "若不能整除，最后一份张量的大小小于其他张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1 tensor: shape is torch.Size([2, 3])\n",
      "No.2 tensor: shape is torch.Size([2, 3])\n",
      "No.3 tensor: shape is torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((2,7))\n",
    "list_of_tensors = torch.chunk(t, dim=1, chunks = 3)\n",
    "\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"No.{} tensor: shape is {}\".format(idx+1, t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.split()**\n",
    "\n",
    "将张量按维度dim进行切分\n",
    "\n",
    "返回张量列表\n",
    "\n",
    "tensor： 要切分的张量\n",
    "\n",
    "split_size_or_sections: 为int时，表示每一份的长度；为list时，按list元素切分\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1 tensor: shape is torch.Size([2, 0])\n",
      "No.2 tensor: shape is torch.Size([2, 2])\n",
      "No.3 tensor: shape is torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,5))\n",
    "list_of_tensors = torch.split(a, dim=1, split_size_or_sections = [0,2,3])\n",
    "\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"No.{} tensor: shape is {}\".format(idx+1, t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量索引\n",
    "\n",
    "**torch.index_select()**\n",
    "\n",
    "在维度dim上，按index索引数据\n",
    "\n",
    "返回按index索引数据拼接的张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([[6, 4, 6],\n",
      "        [4, 2, 0],\n",
      "        [7, 7, 4]])\n",
      "t_select:tensor([[6, 4, 6],\n",
      "        [7, 7, 4]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randint(0,9, size=(3,3))\n",
    "idx = torch.tensor([0,2], dtype=torch.long)    # 数据类型只能时torch.long, 不能为torch.float\n",
    "t_select = torch.index_select(t, dim=0, index=idx)\n",
    "print(\"t:{}\\nt_select:{}\".format(t, t_select))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.masked_select()**\n",
    "\n",
    "按mask中的True进行索引\n",
    "\n",
    "返回一维张量。因为不能确定True的个数，因此也不能确定返回的形状\n",
    "\n",
    "input: 要索引的张量\n",
    "\n",
    "mask：与mask同形状的布尔类型张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([[1, 3, 8],\n",
      "        [8, 4, 0],\n",
      "        [4, 0, 8]])\n",
      "t_select:tensor([8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randint(0,9, size=(3,3))\n",
    "mask = t.ge(5)  #ge, gt, le, lt\n",
    "t_select = torch.masked_select(t, mask)\n",
    "print(\"t:{}\\nt_select:{}\".format(t, t_select))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量变换\n",
    "\n",
    "**torch.reshape()**\n",
    "\n",
    "变换张量形状\n",
    "\n",
    "当张量在内存中是连续时，新张良与input共享数据内存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1024,    1,    5,    4,    7,    0,    6,    2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1024,    1,    5,    4],\n",
       "        [   7,    0,    6,    2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randperm(8)\n",
    "t_reshaped = torch.reshape(t, (2,-1))\n",
    "t[0]=1024\n",
    "t\n",
    "t_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.transpose()**\n",
    "\n",
    "交换张量的两个维度\n",
    "\n",
    "input: 要交换的张量\n",
    "\n",
    "dim0： 要交换的维度\n",
    "\n",
    "dim1： 要交换的维度\n",
    "\n",
    "**torch.t()**\n",
    "\n",
    "2维张量转置，对矩阵而言，等价于\n",
    "\n",
    "torch.t(input, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(2,3,4)\n",
    "t_transposed = torch.transpose(t, dim0=1, dim1=2)\n",
    "t.shape\n",
    "t_transposed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.squeeze()**\n",
    "\n",
    "压缩张量中长度为1的维度（轴）\n",
    "\n",
    "dim:若为None, 移除所有长度为1的轴；若指定维度，当且仅当该轴长度为1时，可以被移除。\n",
    "\n",
    "**torch.unsqueeze()**\n",
    "依据dim扩展维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand((1,2,3,1))\n",
    "t_sq = torch.squeeze(t)\n",
    "t_0 = torch.squeeze(t, dim=0)\n",
    "t_1 = torch.squeeze(t, dim=1)\n",
    "t.shape\n",
    "t_sq.shape\n",
    "t_0.shape\n",
    "t_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的数学运算\n",
    "\n",
    "#### 加法\n",
    "\n",
    "**torch.add(input, other, alpha, out=None)**\n",
    "\n",
    "input: 第一个张量\n",
    "\n",
    "alpha: 乘项因子\n",
    "\n",
    "other: 第二个张量\n",
    "\n",
    "**torch.addcdiv(input, value = 1, tensor1, tensor2, out=None) **\n",
    "\n",
    "$ out_i = input_i + value \\times \\cfrac{tensor1_i}{tensor2_i} $ \n",
    "\n",
    "**torch.addcmul(input, value = 1, tensor1, tensor2, out=None) **\n",
    "\n",
    "$ out_i = input_i + value \\times tensor1_i \\times tensor2_i $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7966, -0.4545, -0.2147],\n",
       "        [ 2.2404,  1.2458,  0.4775],\n",
       "        [-2.1236,  0.8209, -0.6703]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2034,  0.5455,  0.7853],\n",
       "        [ 3.2404,  2.2458,  1.4775],\n",
       "        [-1.1236,  1.8209,  0.3297]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.2034,  9.5455,  9.7853],\n",
       "        [12.2404, 11.2458, 10.4775],\n",
       "        [ 7.8764, 10.8209,  9.3297]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_0 = torch.randn((3,3))\n",
    "t_1 = torch.ones_like(t_0)\n",
    "t_add = torch.add(t_0, t_1)\n",
    "t_add2 = torch.add(t_0,t_1,alpha=10)\n",
    "\n",
    "t_0\n",
    "t_1\n",
    "t_add\n",
    "t_add2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "\n",
    "求解步骤：\n",
    "\n",
    "1. 确定模型Model\n",
    "\n",
    "$ y = wx + b$\n",
    "\n",
    "2. 选择损失函数\n",
    "\n",
    "MSE\n",
    "\n",
    "$ \\cfrac{1}{m}\\sum_{i=1}^m（y_i - \\hat y_i)^2 $\n",
    "\n",
    "3. 求解梯度并更新$w, b$\n",
    "\n",
    "$ w = w - LR \\times w.grad $\n",
    "\n",
    "$ b = b = LR \\times w.grad $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x274461b8430>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3637])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.1880])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0147])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.3747])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5249])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.5258])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6680])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.1058])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.6593])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.4534])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.4023])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.1478])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.3507])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.6936])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.8505])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.3982])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.4668])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.6116])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.4750])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9037])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.3536])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.4051])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.6483])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.1973])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.9894])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.1939])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:12\n",
      " w:[1.193853] b: [3.9894037] loss: 0.6023467183113098\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "# create training data\n",
    "x = torch.rand(20, 1) * 10    # x data (tensor)\n",
    "y = 2 * x + (5 + torch.randn(20,1)) # y data (tensor)\n",
    "\n",
    "w = torch.randn((1), requires_grad = True)\n",
    "b = torch.zeros((1), requires_grad = True)\n",
    "\n",
    "for iteration in range(1000):\n",
    "    # forward propagation\n",
    "    wx = torch.mul(w, x)\n",
    "    y_pred = torch.add(wx, b)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = (0.5 * (y-y_pred)** 2).mean()\n",
    "    \n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update para\n",
    "    b.data.sub_(lr * b.grad)\n",
    "    w.data.sub_(lr * w.grad)\n",
    "    \n",
    "    # drawing\n",
    "#     if iteration % 20 == 0:\n",
    "#             plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "#             plt.plot(x.data.numpy(), y_pred.data.numpy())\n",
    "#             plt.text(2,20, 'Loss=%.4f' % loss.data.numpy())\n",
    "#             plt.xlim(1.5, 10)\n",
    "#             plt.ylim(8, 28)\n",
    "#             plt.title('Iteration:{}\\n w:{} b: {}'.format(iteration, w.data.numpy(), b.data.numpy()))\n",
    "#             plt.pause(0.5)\n",
    "            \n",
    "    if loss.data.numpy() < 1:\n",
    "        break\n",
    "\n",
    "print('Iteration:{}\\n w:{} b: {} loss: {}'.format(iteration, w.data.numpy(), b.data.numpy(), loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_leaf: \n",
      " True True False False False\n",
      "grad_fn: \n",
      " tensor([5.]) tensor([2.]) None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-e19a38a0d7c2>:17: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"grad_fn: \\n\", w.grad, x.grad, a.grad, b.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([1.], requires_grad = True)\n",
    "x = torch.tensor([2.], requires_grad = True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a,b)\n",
    "\n",
    "y.backward()\n",
    "w.grad\n",
    "\n",
    "# check is_leaf\n",
    "print(\"is_leaf: \\n\", w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "\n",
    "# check grad\n",
    "print(\"grad_fn: \\n\", w.grad, x.grad, a.grad, b.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
