{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = (x + w) \\times (w + 1) $\n",
    "\n",
    "$ a = x + w $\n",
    "\n",
    "$ b = w + 1 $\n",
    "\n",
    "$ y = x \\times w $\n",
    "\n",
    "$ \\frac{\\partial y}{\\partial w} = \\frac{\\partial y}{\\partial a}\\frac{\\partial a}{\\partial w} + \\frac{\\partial y}{\\partial b}\\frac{\\partial b}{\\partial w} \n",
    "= b \\times 1 + a \\times 1\n",
    "= (w+1)+(x+w)\n",
    "= 2w + x + 1$\n",
    "\n",
    "$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial a}\\frac{\\partial a}{\\partial x} + \\frac{\\partial y}{\\partial b}\\frac{\\partial b}{\\partial x} \n",
    "= b \\times 1 + a \\times 0\n",
    "= w+1 $\n",
    "\n",
    "**confirm by following codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([5.])\n",
      "x.grad= tensor([2.])\n",
      "is_leaf: x, w, a, b, y:\n",
      " True True False False False\n",
      "gradient: x, w, a, b, y:\n",
      " tensor([2.]) tensor([5.]) tensor([2.]) None None\n",
      "grad func: x, w, a, b, y:\n",
      " None None <AddBackward0 object at 0x0000014F39715190> <AddBackward0 object at 0x0000014F39715160> <MulBackward0 object at 0x0000014F39715A00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-841275c4e00b>:19: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"gradient: x, w, a, b, y:\\n\", x.grad, w.grad, a.grad, b.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.], requires_grad = True)\n",
    "w = torch.tensor([1.], requires_grad = True)\n",
    "\n",
    "a = torch.add(x, w)    \n",
    "a.retain_grad()\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "print(f'w.grad= {w.grad}')\n",
    "print(f'x.grad= {x.grad}')\n",
    "\n",
    "# check leaf nodes\n",
    "print(\"is_leaf: x, w, a, b, y:\\n\", x.is_leaf, w.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "\n",
    "# check gradient\n",
    "print(\"gradient: x, w, a, b, y:\\n\", x.grad, w.grad, a.grad, b.grad, y.grad)\n",
    "\n",
    "# check grad_fn\n",
    "print(\"grad func: x, w, a, b, y:\\n\", x.grad_fn, w.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False)\n",
    "\n",
    "- tensors: 用于求导的张量，如loss\n",
    "- grad_tensors: 多梯度权重\n",
    "- retain_graph: 保存计算图\n",
    "- create_graph: 创建导数计算图，用于高阶求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([10.])\n",
      "x.grad= tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(10)\n",
    "\n",
    "x = torch.tensor([2.], requires_grad = True)\n",
    "w = torch.tensor([1.], requires_grad = True)\n",
    "\n",
    "a = torch.add(x, w)    \n",
    "a.retain_grad()\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "# y.backward(retain_graph = True)\n",
    "torch.autograd.backward(y, retain_graph= True)\n",
    "y.backward(retain_graph= True)\n",
    "\n",
    "print(f'w.grad= {w.grad}')\n",
    "print(f'x.grad= {x.grad}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grad_tensors的使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([7.])\n",
      "x.grad= tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(10)\n",
    "\n",
    "x = torch.tensor([2.], requires_grad = True)\n",
    "w = torch.tensor([1.], requires_grad = True)\n",
    "\n",
    "a = torch.add(x, w)    \n",
    "b = torch.add(w, 1)\n",
    "y0 = torch.mul(a, b)\n",
    "y1 = torch.add(a, b)    #dy1/dw =2, dy1/dx = 1\n",
    "\n",
    "loss = torch.cat([y0,y1], dim=0)\n",
    "grad_tensors = torch.tensor([1.,1.])\n",
    "\n",
    "#loss.backward(gradient = grad_tensors) # gradient 传入torch.autograd.backward()中的grad_tensors\n",
    "torch.autograd.backward(loss, grad_tensors = grad_tensors, retain_graph= True)\n",
    "\n",
    "\n",
    "print(f'w.grad= {w.grad}')\n",
    "print(f'x.grad= {x.grad}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.autograd.grad(outputs, inputs, grad_outputs = None, retain_graph=None, create_graph=False)\n",
    "\n",
    "- outputs: 用于求导的张量，如loss\n",
    "- inputs: 需要梯度的张量\n",
    "- grad_tensors: 多梯度权重\n",
    "- retain_graph: 保存计算图\n",
    "- create_graph: 创建导数计算图，用于高阶求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st grad= (tensor([6.], grad_fn=<MulBackward0>),)\n",
      "2nd grad= (tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(10)\n",
    "\n",
    "x = torch.tensor([3.], requires_grad = True)\n",
    "y = torch.pow(x, 2)    # y=x^2\n",
    "\n",
    "grad_1 = torch.autograd.grad(y, x, create_graph= True)    #创建了导数计算图才能计算二阶导数\n",
    "grad_2 = torch.autograd.grad(grad_1[0], x)\n",
    "\n",
    "print(f'1st grad= {grad_1}')\n",
    "print(f'2nd grad= {grad_2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度不自动清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([5.])\n",
      "w.grad= tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.], requires_grad = True)\n",
    "w = torch.tensor([1.], requires_grad = True)\n",
    "\n",
    "for i in range(2):\n",
    "    a = torch.add(x, w)    \n",
    "    \n",
    "    b = torch.add(w, 1)\n",
    "    y = torch.mul(a, b)\n",
    "\n",
    "    y.backward()\n",
    "    print(f'w.grad= {w.grad}')\n",
    "    \n",
    "    w.grad.zero_()    #如果不手动清零，则梯度每次会累加\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依赖叶子节点的节点，requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad= True b.requires_grad= True y.requires_grad= True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.], requires_grad = True)\n",
    "w = torch.tensor([1.], requires_grad = True)\n",
    "\n",
    "a = torch.add(x, w)    \n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "print(f'a.requires_grad= {a.requires_grad} b.requires_grad= {b.requires_grad} y.requires_grad= {y.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 叶子节点不能执行原位操作"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
